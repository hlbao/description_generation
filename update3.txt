Really an evaluation of your working model using the evaluation metrics that make sense for your task, 

this includes accuracy, Precision, Recall, F1, 

or a metric specific for your task if these metrics do not work for you, for example BLEU for MT, etc.

For most of the projects, it makes sense to try a number of models and compare the results. 

Describe what algorithms/models you have used and via which tools.

Evaluate various models and report performance of your model on your test data. 

You can provide the evaluation of your test data or use cross-validation. 

It would be good to communicate about this step 

and let me know what is your approach to improving your initial results. 


