Some of you already have a working model after the previous assignment, and some still have not completed that. For the next deadline, I would like to see some results from your models. So, what I need is really an evaluation of your working model using the evaluation metrics that make sense for your task, this includes accuracy, Precision, Recall, F1, or a metric specific for your task if these metrics do not work for you, for example BLEU for MT, etc.

For most of the projects, it makes sense to try a number of models and compare the results. Describe what algorithms/models you have used and via which tools.  Evaluate various models and report performance of your model on your test data. You can provide the evaluation of your test data or use cross-validation. It would be good to communicate about this step and let me know what is your approach to improving your initial results. Post your main test results on D2L and the codes on your GitHub repository with the link in here.
